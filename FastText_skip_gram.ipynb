{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chuẩn bị dữ liệu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_piece(word, n_gram=3):\n",
    "    word_pieces = []\n",
    "    word = \"<\"+word+\">\"\n",
    "    for index, w in enumerate(word):\n",
    "        if index<=len(word)-n_gram:\n",
    "            word_pieces.append(word[index:index+n_gram])\n",
    "    return word_pieces\n",
    "\n",
    "def tokenize(text):\n",
    "    # obtains tokens with a least 1 alphabet\n",
    "    pattern = re.compile(r'[A-Za-z]+[\\w^\\']*|[\\w^\\']*[A-Za-z]+[\\w^\\']*')\n",
    "    word_tokens = pattern.findall(text.lower())\n",
    "    word_piece_tokens = []\n",
    "    for word_token in word_tokens:\n",
    "        word_piece_token = get_word_piece(word_token)\n",
    "        for piece in word_piece_token:\n",
    "            word_piece_tokens.append(piece)\n",
    "    return word_piece_tokens\n",
    "\n",
    "\n",
    "def mapping(tokens):\n",
    "    word_to_id = dict()\n",
    "    id_to_word = dict()\n",
    "\n",
    "    for i, token in enumerate(set(tokens)):\n",
    "        word_to_id[token] = i\n",
    "        id_to_word[i] = token\n",
    "\n",
    "    return word_to_id, id_to_word\n",
    "\n",
    "def generate_training_data(tokens, word_to_id, window_size):\n",
    "    N = len(tokens)\n",
    "    X, Y = [], []\n",
    "\n",
    "    for i in range(N):\n",
    "        nbr_inds = list(range(max(0, i - window_size), i)) + \\\n",
    "                   list(range(i + 1, min(N, i + window_size + 1)))\n",
    "        for j in nbr_inds:\n",
    "            X.append(word_to_id[tokens[i]])\n",
    "            Y.append(word_to_id[tokens[j]])\n",
    "            \n",
    "    X = np.array(X)\n",
    "#     X = np.expand_dims(X, axis=0)\n",
    "    Y = np.array(Y)\n",
    "#     Y = np.expand_dims(Y, axis=0)\n",
    "            \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<af',\n",
       " 'aft',\n",
       " 'fte',\n",
       " 'ter',\n",
       " 'er>',\n",
       " '<th',\n",
       " 'the',\n",
       " 'he>',\n",
       " '<de',\n",
       " 'ded',\n",
       " 'edu',\n",
       " 'duc',\n",
       " 'uct',\n",
       " 'cti',\n",
       " 'tio',\n",
       " 'ion',\n",
       " 'on>',\n",
       " '<of',\n",
       " 'of>',\n",
       " '<th',\n",
       " 'the',\n",
       " 'he>',\n",
       " '<co',\n",
       " 'cos',\n",
       " 'ost',\n",
       " 'sts',\n",
       " 'ts>',\n",
       " '<of',\n",
       " 'of>',\n",
       " '<hi',\n",
       " 'his',\n",
       " 'is>',\n",
       " '<in',\n",
       " 'inv',\n",
       " 'nve',\n",
       " 'ves',\n",
       " 'est',\n",
       " 'sti',\n",
       " 'tin',\n",
       " 'ing',\n",
       " 'ng>',\n",
       " '<co',\n",
       " 'cos',\n",
       " 'osi',\n",
       " 'sin',\n",
       " 'in>',\n",
       " '<si',\n",
       " 'sim',\n",
       " 'imi',\n",
       " 'mil',\n",
       " 'ila',\n",
       " 'lar',\n",
       " 'ari',\n",
       " 'rit',\n",
       " 'ity',\n",
       " 'ty>',\n",
       " '<be',\n",
       " 'bea',\n",
       " 'eat',\n",
       " 'ati',\n",
       " 'tin',\n",
       " 'ing',\n",
       " 'ng>',\n",
       " '<th',\n",
       " 'the',\n",
       " 'he>',\n",
       " '<st',\n",
       " 'sto',\n",
       " 'toc',\n",
       " 'ock',\n",
       " 'ck>',\n",
       " '<ma',\n",
       " 'mar',\n",
       " 'ark',\n",
       " 'rke',\n",
       " 'ket',\n",
       " 'et>',\n",
       " '<is',\n",
       " 'is>',\n",
       " '<a>',\n",
       " '<lo',\n",
       " 'los',\n",
       " 'ose',\n",
       " 'ser',\n",
       " \"er'\",\n",
       " \"r's\",\n",
       " \"'s>\",\n",
       " '<ga',\n",
       " 'gam',\n",
       " 'ame',\n",
       " 'me>']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = \"After the deduction of the costs of his investing, \" \\\n",
    "      \"Cosin similarity beating the stock market is a loser's game.\"\n",
    "tokens = tokenize(doc)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<af', 'aft', 'fte', 'ter', 'er>']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_word_piece(\"after\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ves': 0,\n",
       " '<in': 1,\n",
       " 'ame': 2,\n",
       " 'ost': 3,\n",
       " 'est': 4,\n",
       " 'inv': 5,\n",
       " '<th': 6,\n",
       " 'ing': 7,\n",
       " 'of>': 8,\n",
       " 'mil': 9,\n",
       " 'tin': 10,\n",
       " 'lar': 11,\n",
       " 'rit': 12,\n",
       " 'his': 13,\n",
       " 'the': 14,\n",
       " 'ty>': 15,\n",
       " 'toc': 16,\n",
       " \"'s>\": 17,\n",
       " 'me>': 18,\n",
       " 'sts': 19,\n",
       " '<hi': 20,\n",
       " 'gam': 21,\n",
       " 'duc': 22,\n",
       " 'cti': 23,\n",
       " 'on>': 24,\n",
       " 'mar': 25,\n",
       " 'nve': 26,\n",
       " '<ga': 27,\n",
       " 'ark': 28,\n",
       " 'ck>': 29,\n",
       " 'edu': 30,\n",
       " 'ati': 31,\n",
       " '<is': 32,\n",
       " 'in>': 33,\n",
       " '<st': 34,\n",
       " '<co': 35,\n",
       " 'tio': 36,\n",
       " '<ma': 37,\n",
       " 'sim': 38,\n",
       " '<lo': 39,\n",
       " 'is>': 40,\n",
       " 'ose': 41,\n",
       " 'er>': 42,\n",
       " 'ter': 43,\n",
       " 'ng>': 44,\n",
       " 'osi': 45,\n",
       " '<si': 46,\n",
       " 'eat': 47,\n",
       " 'los': 48,\n",
       " \"r's\": 49,\n",
       " '<a>': 50,\n",
       " 'aft': 51,\n",
       " '<of': 52,\n",
       " 'sin': 53,\n",
       " 'sti': 54,\n",
       " 'rke': 55,\n",
       " \"er'\": 56,\n",
       " '<af': 57,\n",
       " 'ity': 58,\n",
       " 'ts>': 59,\n",
       " 'ock': 60,\n",
       " 'ded': 61,\n",
       " 'uct': 62,\n",
       " 'ket': 63,\n",
       " 'et>': 64,\n",
       " 'cos': 65,\n",
       " 'fte': 66,\n",
       " 'sto': 67,\n",
       " 'he>': 68,\n",
       " 'ion': 69,\n",
       " 'imi': 70,\n",
       " '<be': 71,\n",
       " 'bea': 72,\n",
       " '<de': 73,\n",
       " 'ser': 74,\n",
       " 'ari': 75,\n",
       " 'ila': 76}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_id, id_to_word = mapping(tokens)\n",
    "word_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(534,)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, Y = generate_training_data(tokens, word_to_id, 3)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(534,)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<af --> aft\n",
      "<af --> fte\n",
      "<af --> ter\n",
      "aft --> <af\n",
      "aft --> fte\n",
      "aft --> ter\n",
      "aft --> er>\n",
      "fte --> <af\n",
      "fte --> aft\n",
      "fte --> ter\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(id_to_word[X[i]],\"-->\", id_to_word[Y[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([57, 57, 57, 51, 51])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([51, 66, 43, 57, 66])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "X_train = to_categorical(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train = to_categorical(Y)\n",
    "Y_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(534, 77) (534, 77)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, Y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huấn luyện "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size_of_vocab = X_train.shape[1]\n",
    "size_of_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_size = 112"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(emb_size, activation='linear', input_dim=size_of_vocab ))\n",
    "model.add(Dense(size_of_vocab, activation='linear'))\n",
    "model.add(Activation(\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 112)               8736      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 77)                8701      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 77)                0         \n",
      "=================================================================\n",
      "Total params: 17,437\n",
      "Trainable params: 17,437\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(lr=0.001), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "534/534 [==============================] - 0s 771us/step - loss: 0.0694 - acc: 0.9870\n",
      "Epoch 2/100\n",
      "534/534 [==============================] - 0s 157us/step - loss: 0.0685 - acc: 0.9870\n",
      "Epoch 3/100\n",
      "534/534 [==============================] - 0s 164us/step - loss: 0.0675 - acc: 0.9870\n",
      "Epoch 4/100\n",
      "534/534 [==============================] - 0s 161us/step - loss: 0.0665 - acc: 0.9870\n",
      "Epoch 5/100\n",
      "534/534 [==============================] - 0s 157us/step - loss: 0.0654 - acc: 0.9870\n",
      "Epoch 6/100\n",
      "534/534 [==============================] - 0s 223us/step - loss: 0.0642 - acc: 0.9870\n",
      "Epoch 7/100\n",
      "534/534 [==============================] - 0s 178us/step - loss: 0.0628 - acc: 0.9870\n",
      "Epoch 8/100\n",
      "534/534 [==============================] - 0s 160us/step - loss: 0.0613 - acc: 0.9870\n",
      "Epoch 9/100\n",
      "534/534 [==============================] - 0s 154us/step - loss: 0.0597 - acc: 0.9870\n",
      "Epoch 10/100\n",
      "534/534 [==============================] - 0s 162us/step - loss: 0.0579 - acc: 0.9870\n",
      "Epoch 11/100\n",
      "534/534 [==============================] - 0s 158us/step - loss: 0.0561 - acc: 0.9870\n",
      "Epoch 12/100\n",
      "534/534 [==============================] - 0s 159us/step - loss: 0.0542 - acc: 0.9870\n",
      "Epoch 13/100\n",
      "534/534 [==============================] - 0s 153us/step - loss: 0.0524 - acc: 0.9870\n",
      "Epoch 14/100\n",
      "534/534 [==============================] - 0s 160us/step - loss: 0.0507 - acc: 0.9870\n",
      "Epoch 15/100\n",
      "534/534 [==============================] - 0s 155us/step - loss: 0.0491 - acc: 0.9870\n",
      "Epoch 16/100\n",
      "534/534 [==============================] - 0s 154us/step - loss: 0.0477 - acc: 0.9870\n",
      "Epoch 17/100\n",
      "534/534 [==============================] - 0s 167us/step - loss: 0.0465 - acc: 0.9870\n",
      "Epoch 18/100\n",
      "534/534 [==============================] - 0s 169us/step - loss: 0.0455 - acc: 0.9870\n",
      "Epoch 19/100\n",
      "534/534 [==============================] - 0s 154us/step - loss: 0.0446 - acc: 0.9870\n",
      "Epoch 20/100\n",
      "534/534 [==============================] - 0s 170us/step - loss: 0.0439 - acc: 0.9870\n",
      "Epoch 21/100\n",
      "534/534 [==============================] - 0s 164us/step - loss: 0.0433 - acc: 0.9870\n",
      "Epoch 22/100\n",
      "534/534 [==============================] - 0s 164us/step - loss: 0.0428 - acc: 0.9870\n",
      "Epoch 23/100\n",
      "534/534 [==============================] - 0s 190us/step - loss: 0.0424 - acc: 0.9870\n",
      "Epoch 24/100\n",
      "534/534 [==============================] - 0s 215us/step - loss: 0.0420 - acc: 0.9870\n",
      "Epoch 25/100\n",
      "534/534 [==============================] - 0s 207us/step - loss: 0.0417 - acc: 0.9870\n",
      "Epoch 26/100\n",
      "534/534 [==============================] - 0s 157us/step - loss: 0.0415 - acc: 0.9870\n",
      "Epoch 27/100\n",
      "534/534 [==============================] - 0s 171us/step - loss: 0.0412 - acc: 0.9870\n",
      "Epoch 28/100\n",
      "534/534 [==============================] - 0s 205us/step - loss: 0.0410 - acc: 0.9870\n",
      "Epoch 29/100\n",
      "534/534 [==============================] - 0s 184us/step - loss: 0.0409 - acc: 0.9870\n",
      "Epoch 30/100\n",
      "534/534 [==============================] - 0s 148us/step - loss: 0.0407 - acc: 0.9870\n",
      "Epoch 31/100\n",
      "534/534 [==============================] - 0s 168us/step - loss: 0.0405 - acc: 0.9870\n",
      "Epoch 32/100\n",
      "534/534 [==============================] - 0s 206us/step - loss: 0.0404 - acc: 0.9870\n",
      "Epoch 33/100\n",
      "534/534 [==============================] - 0s 194us/step - loss: 0.0403 - acc: 0.9870\n",
      "Epoch 34/100\n",
      "534/534 [==============================] - 0s 157us/step - loss: 0.0401 - acc: 0.9870\n",
      "Epoch 35/100\n",
      "534/534 [==============================] - 0s 157us/step - loss: 0.0401 - acc: 0.9870\n",
      "Epoch 36/100\n",
      "534/534 [==============================] - 0s 158us/step - loss: 0.0400 - acc: 0.9870\n",
      "Epoch 37/100\n",
      "534/534 [==============================] - 0s 167us/step - loss: 0.0399 - acc: 0.9870\n",
      "Epoch 38/100\n",
      "534/534 [==============================] - 0s 152us/step - loss: 0.0398 - acc: 0.9870\n",
      "Epoch 39/100\n",
      "534/534 [==============================] - 0s 166us/step - loss: 0.0397 - acc: 0.9870\n",
      "Epoch 40/100\n",
      "534/534 [==============================] - 0s 162us/step - loss: 0.0397 - acc: 0.9870\n",
      "Epoch 41/100\n",
      "534/534 [==============================] - 0s 158us/step - loss: 0.0396 - acc: 0.9870\n",
      "Epoch 42/100\n",
      "534/534 [==============================] - 0s 161us/step - loss: 0.0396 - acc: 0.9870\n",
      "Epoch 43/100\n",
      "534/534 [==============================] - 0s 171us/step - loss: 0.0395 - acc: 0.9870\n",
      "Epoch 44/100\n",
      "534/534 [==============================] - 0s 171us/step - loss: 0.0395 - acc: 0.9870\n",
      "Epoch 45/100\n",
      "534/534 [==============================] - 0s 197us/step - loss: 0.0394 - acc: 0.9870\n",
      "Epoch 46/100\n",
      "534/534 [==============================] - 0s 197us/step - loss: 0.0394 - acc: 0.9870\n",
      "Epoch 47/100\n",
      "534/534 [==============================] - 0s 179us/step - loss: 0.0393 - acc: 0.9870\n",
      "Epoch 48/100\n",
      "534/534 [==============================] - 0s 149us/step - loss: 0.0393 - acc: 0.9870\n",
      "Epoch 49/100\n",
      "534/534 [==============================] - 0s 181us/step - loss: 0.0392 - acc: 0.9870\n",
      "Epoch 50/100\n",
      "534/534 [==============================] - 0s 198us/step - loss: 0.0392 - acc: 0.9870\n",
      "Epoch 51/100\n",
      "534/534 [==============================] - 0s 178us/step - loss: 0.0392 - acc: 0.9870\n",
      "Epoch 52/100\n",
      "534/534 [==============================] - 0s 154us/step - loss: 0.0391 - acc: 0.9870\n",
      "Epoch 53/100\n",
      "534/534 [==============================] - 0s 159us/step - loss: 0.0391 - acc: 0.9870\n",
      "Epoch 54/100\n",
      "534/534 [==============================] - 0s 149us/step - loss: 0.0391 - acc: 0.9870\n",
      "Epoch 55/100\n",
      "534/534 [==============================] - 0s 149us/step - loss: 0.0390 - acc: 0.9870\n",
      "Epoch 56/100\n",
      "534/534 [==============================] - 0s 148us/step - loss: 0.0391 - acc: 0.9870\n",
      "Epoch 57/100\n",
      "534/534 [==============================] - 0s 152us/step - loss: 0.0390 - acc: 0.9870\n",
      "Epoch 58/100\n",
      "534/534 [==============================] - 0s 150us/step - loss: 0.0390 - acc: 0.9870\n",
      "Epoch 59/100\n",
      "534/534 [==============================] - 0s 154us/step - loss: 0.0390 - acc: 0.9870\n",
      "Epoch 60/100\n",
      "534/534 [==============================] - 0s 153us/step - loss: 0.0389 - acc: 0.9870\n",
      "Epoch 61/100\n",
      "534/534 [==============================] - 0s 152us/step - loss: 0.0389 - acc: 0.9870\n",
      "Epoch 62/100\n",
      "534/534 [==============================] - 0s 195us/step - loss: 0.0389 - acc: 0.9870\n",
      "Epoch 63/100\n",
      "534/534 [==============================] - 0s 199us/step - loss: 0.0389 - acc: 0.9870\n",
      "Epoch 64/100\n",
      "534/534 [==============================] - 0s 151us/step - loss: 0.0389 - acc: 0.9870\n",
      "Epoch 65/100\n",
      "534/534 [==============================] - 0s 182us/step - loss: 0.0389 - acc: 0.9870\n",
      "Epoch 66/100\n",
      "534/534 [==============================] - 0s 206us/step - loss: 0.0389 - acc: 0.9870\n",
      "Epoch 67/100\n",
      "534/534 [==============================] - 0s 171us/step - loss: 0.0388 - acc: 0.9870\n",
      "Epoch 68/100\n",
      "534/534 [==============================] - 0s 153us/step - loss: 0.0388 - acc: 0.9870\n",
      "Epoch 69/100\n",
      "534/534 [==============================] - 0s 200us/step - loss: 0.0388 - acc: 0.9870\n",
      "Epoch 70/100\n",
      "534/534 [==============================] - 0s 209us/step - loss: 0.0388 - acc: 0.9870\n",
      "Epoch 71/100\n",
      "534/534 [==============================] - 0s 175us/step - loss: 0.0388 - acc: 0.9870\n",
      "Epoch 72/100\n",
      "534/534 [==============================] - 0s 146us/step - loss: 0.0388 - acc: 0.9870\n",
      "Epoch 73/100\n",
      "534/534 [==============================] - 0s 161us/step - loss: 0.0388 - acc: 0.9870\n",
      "Epoch 74/100\n",
      "534/534 [==============================] - 0s 158us/step - loss: 0.0387 - acc: 0.9870\n",
      "Epoch 75/100\n",
      "534/534 [==============================] - 0s 169us/step - loss: 0.0387 - acc: 0.9870\n",
      "Epoch 76/100\n",
      "534/534 [==============================] - 0s 159us/step - loss: 0.0387 - acc: 0.9870\n",
      "Epoch 77/100\n",
      "534/534 [==============================] - 0s 166us/step - loss: 0.0387 - acc: 0.9870\n",
      "Epoch 78/100\n",
      "534/534 [==============================] - 0s 171us/step - loss: 0.0387 - acc: 0.9870\n",
      "Epoch 79/100\n",
      "534/534 [==============================] - 0s 173us/step - loss: 0.0387 - acc: 0.9870\n",
      "Epoch 80/100\n",
      "534/534 [==============================] - 0s 161us/step - loss: 0.0387 - acc: 0.9870\n",
      "Epoch 81/100\n",
      "534/534 [==============================] - 0s 172us/step - loss: 0.0387 - acc: 0.9870\n",
      "Epoch 82/100\n",
      "534/534 [==============================] - 0s 178us/step - loss: 0.0387 - acc: 0.9870\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "534/534 [==============================] - 0s 155us/step - loss: 0.0386 - acc: 0.9870\n",
      "Epoch 84/100\n",
      "534/534 [==============================] - 0s 165us/step - loss: 0.0386 - acc: 0.9870\n",
      "Epoch 85/100\n",
      "534/534 [==============================] - 0s 178us/step - loss: 0.0386 - acc: 0.9870\n",
      "Epoch 86/100\n",
      "534/534 [==============================] - 0s 164us/step - loss: 0.0386 - acc: 0.9870\n",
      "Epoch 87/100\n",
      "534/534 [==============================] - 0s 152us/step - loss: 0.0387 - acc: 0.9870\n",
      "Epoch 88/100\n",
      "534/534 [==============================] - 0s 164us/step - loss: 0.0386 - acc: 0.9870\n",
      "Epoch 89/100\n",
      "534/534 [==============================] - 0s 173us/step - loss: 0.0386 - acc: 0.9870\n",
      "Epoch 90/100\n",
      "534/534 [==============================] - 0s 151us/step - loss: 0.0386 - acc: 0.9870\n",
      "Epoch 91/100\n",
      "534/534 [==============================] - 0s 175us/step - loss: 0.0386 - acc: 0.9870\n",
      "Epoch 92/100\n",
      "534/534 [==============================] - 0s 175us/step - loss: 0.0386 - acc: 0.9870\n",
      "Epoch 93/100\n",
      "534/534 [==============================] - 0s 165us/step - loss: 0.0386 - acc: 0.9870\n",
      "Epoch 94/100\n",
      "534/534 [==============================] - 0s 159us/step - loss: 0.0386 - acc: 0.9870\n",
      "Epoch 95/100\n",
      "534/534 [==============================] - 0s 168us/step - loss: 0.0386 - acc: 0.9870\n",
      "Epoch 96/100\n",
      "534/534 [==============================] - 0s 170us/step - loss: 0.0386 - acc: 0.9870\n",
      "Epoch 97/100\n",
      "534/534 [==============================] - 0s 165us/step - loss: 0.0386 - acc: 0.9870\n",
      "Epoch 98/100\n",
      "534/534 [==============================] - 0s 160us/step - loss: 0.0386 - acc: 0.9870\n",
      "Epoch 99/100\n",
      "534/534 [==============================] - 0s 166us/step - loss: 0.0385 - acc: 0.9870\n",
      "Epoch 100/100\n",
      "534/534 [==============================] - 0s 174us/step - loss: 0.0386 - acc: 0.9870\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f25082b7fd0>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, epochs=100, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get trained word embedding weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "Word_embeding = model.get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.29162848, -0.07783902, -0.2280984 , ...,  0.5298977 ,\n",
       "        -0.44802997, -0.70827174],\n",
       "       [-0.01633759, -0.3140511 , -0.28413194, ..., -0.19401434,\n",
       "        -0.35291213, -0.07612595],\n",
       "       [ 0.23913978, -0.505189  ,  0.23383209, ..., -0.12183629,\n",
       "         0.14598939,  0.0737638 ],\n",
       "       ...,\n",
       "       [-0.409593  , -0.24502864,  0.11214375, ..., -0.01677183,\n",
       "         0.26012078, -0.32655013],\n",
       "       [ 0.271678  ,  0.40443122, -0.03090577, ...,  0.36683407,\n",
       "        -0.42701516,  0.5837345 ],\n",
       "       [ 0.18708327,  0.1078411 , -0.2054083 , ...,  0.15078776,\n",
       "        -0.14029126,  0.51986206]], dtype=float32)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Word_embeding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing get embedding of the word \"after\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77, 112)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Word_embeding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.01444846 -0.431602   -0.00264669  0.17870767  0.3952416   0.27600938\n",
      "  0.14046016 -0.36320776  0.19632185 -0.16066098  0.01900725 -0.1814755\n",
      " -0.12838139 -0.22161512 -0.00749634  0.26762637 -0.20738968 -0.085875\n",
      " -0.0401297   0.2765072  -0.2681163   0.28341728  0.15625647 -0.36464217\n",
      "  0.1364804  -0.29135412 -0.36955887 -0.03564329  0.33625397  0.1507515\n",
      "  0.00240508 -0.05599285 -0.00340888 -0.323869    0.3481112  -0.14642613\n",
      " -0.11769992  0.312607   -0.17820561  0.05731595  0.24385338  0.41541559\n",
      " -0.23995419  0.520674    0.04787733  0.05834616 -0.11685461 -0.24486664\n",
      " -0.1847879   0.34670573 -0.02185403  0.3039195   0.22808722  0.31143373\n",
      " -0.20058432  0.29002178 -0.13666964 -0.5770857  -0.37277004  0.3246476\n",
      "  0.00832421 -0.4108111   0.5126812  -0.16593562  0.18165083 -0.4060672\n",
      "  0.2746221  -0.3246849   0.17303386  0.20828936  0.030694   -0.32349133\n",
      "  0.17342229  0.362675    0.01897696 -0.34025058 -0.10872617  0.22097048\n",
      "  0.1455799   0.07556392  0.6033536  -0.27384478 -0.20572186 -0.17849244\n",
      " -0.05015589 -0.6024092  -0.16782191 -0.07705209 -0.13049355  0.08623312\n",
      "  0.01205485  0.44228014  0.3295275  -0.29073703  0.0710858  -0.20079923\n",
      "  0.15177388 -0.40756923 -0.20675555  0.00451886  0.25261122  0.2558418\n",
      " -0.06408715  0.03940117  0.37484434  0.19508371 -0.2671798  -0.0909198\n",
      "  0.03794395  0.1365503   0.10248786 -0.32865137]\n"
     ]
    }
   ],
   "source": [
    "embbeding = None\n",
    "word_pieces = get_word_piece(\"after\", n_gram=3)\n",
    "for piece in word_pieces:\n",
    "    piece_id = word_to_id[piece]\n",
    "    if embbeding is None:\n",
    "        embbeding = Word_embeding[piece_id]\n",
    "    else:\n",
    "        embbeding += Word_embeding[piece_id]\n",
    "embbeding = embbeding/len(word_pieces)\n",
    "print(embbeding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing get embedding of word \"losing\" not in the dictionary but it's word piece existed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<lo', 'los', 'osi', 'sin', 'ing', 'ng>']\n",
      "<lo 39\n",
      "los 48\n",
      "osi 45\n",
      "sin 53\n",
      "ing 7\n",
      "ng> 44\n",
      "[ 0.5233029  -0.18932785  0.01923718 -0.6361206   0.2346127   0.37669668\n",
      " -0.7926662  -0.1076906   0.3485751   0.7007458   1.0222338  -0.981503\n",
      " -0.29068643  0.43090343 -0.85105735  0.5052257  -0.5531355   0.23221801\n",
      "  0.831013    0.18873595  0.1506164   0.28947803  0.5842615   0.5527968\n",
      " -0.18634443  1.1940554  -0.12421032  0.3061869   0.02212839  0.09907737\n",
      "  0.8860903  -0.89186317  0.52304065  0.7043198  -0.21277428 -0.13694693\n",
      "  0.6182614   0.0039833   0.40062726 -1.059151    0.44069275 -0.20310889\n",
      "  0.06289941 -0.7391157  -0.6868355  -0.62666565 -0.7596021   0.56353647\n",
      "  0.53589016 -0.40051767  0.91064864  0.5102177   0.00672227  0.65918183\n",
      "  1.21952    -0.94920826 -0.9105738   0.75623727  0.01226761  1.3328598\n",
      "  1.2092388  -0.43106627 -0.31395558 -0.18870063 -0.04480935  0.26839593\n",
      " -0.61750036  0.85443044 -0.00763851  1.1012146  -0.61211395  0.59797055\n",
      "  0.41346368 -0.16219763 -0.47783998 -0.23970777  0.8303836   0.87623423\n",
      " -0.12861343  0.81683415 -0.15689032  0.0191737  -0.01107643  0.55690056\n",
      "  0.41023433  0.51987976  0.00691456 -0.25966245  0.60891706  0.8283375\n",
      " -0.22177692 -1.2263178   0.35498902 -0.47867712 -0.77197653  0.4389281\n",
      " -0.1814416   0.5518407  -0.62161154 -0.71376723  1.1257938   0.0720612\n",
      "  0.09992868 -0.32857135  0.5337082  -0.3257065  -0.01809297 -0.38928962\n",
      "  0.14865386 -0.31610474  0.80044883 -1.0115181 ]\n"
     ]
    }
   ],
   "source": [
    "embbeding = None\n",
    "word_pieces = get_word_piece(\"losing\", n_gram=3)\n",
    "print(word_pieces)\n",
    "for word_piece in word_pieces:\n",
    "    print(word_piece, word_to_id[word_piece])\n",
    "for piece in word_pieces:\n",
    "    piece_id = word_to_id[piece]\n",
    "    if embbeding is None:\n",
    "        embbeding = Word_embeding[piece_id]\n",
    "    else:\n",
    "        embbeding += Word_embeding[piece_id]\n",
    "embbeding = embbeding/len(word_pieces)\n",
    "print(embbeding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
